{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import torch\n",
    "from torch import nn\n",
    "from random import randint\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "'''\n",
    "Declare model parameters\n",
    "'''\n",
    "vocab_size = 83\n",
    "embedding_dim = 256\n",
    "rnn_units = 1024\n",
    "batch_size = 4\n",
    "seq_length = 100"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "'''\n",
    "Create an embedding layer in keras\n",
    "'''\n",
    "k_emb = tf.keras.layers.Embedding(vocab_size, embedding_dim, batch_input_shape=[batch_size, None])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "'''\n",
    "Create input for keras model\n",
    "'''\n",
    "k_input = np.array([[randint(0,vocab_size-1) for j in range(seq_length)] for i in range(batch_size)])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "'''\n",
    "Get output of embedding layer\n",
    "'''\n",
    "k_test = k_emb(k_input)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "'''\n",
    "Create an embedding layer in PyTorch\n",
    "'''\n",
    "p_emb = torch.nn.Embedding(batch_size*seq_length, embedding_dim)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "'''\n",
    "Use the same input we used for keras but make it into a PyTorch transformer\n",
    "'''\n",
    "p_input = torch.tensor(k_input)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "'''\n",
    "Get output from embedding layer\n",
    "'''\n",
    "p_test = p_emb(p_input)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "'''\n",
    "During forward propagation, the Pytorch LSTM layer expects a 3 dimensional tensor as input (similar to TF).\n",
    "But the default ordering of the dimensions changes. The input tensor should be of shape (timesteps, batch, input_features).\n",
    "If we want to get the same order of dimensions as TF, we should set batch_first=True at layer initiation.\n",
    "'''\n",
    "p_lstm = torch.nn.LSTM(embedding_dim, batch_size*embedding_dim, batch_first=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "'''\n",
    "Get output of PyTorch LSTM\n",
    "'''\n",
    "p_lstm_test = p_lstm(p_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([4, 100, 1024])"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "The output of the Pytorch LSTM layer is a tuple with two elements.\n",
    "The first element of the tuple is LSTM’s output corresponding to all timesteps (hᵗ : ∀t = 1,2…T)\n",
    "with shape (timesteps, batch, output_features).\n",
    "The second element of the tuple is another tuple with two elements.\n",
    "The first element of this second tuple is the output corresponding to the last timestep (hᵀ).\n",
    "It has the shape (1, batch, output_features).\n",
    "The second element of this second tuple is the cell state corresponding to the last timestep (cᵀ).\n",
    "It also has the shape (1, batch, output_features).\n",
    "If we had initiated the LSTM as a block of stacked layers by setting num_layers=k,\n",
    "then hᵀ and cᵀ would have the shape (k, batch, output_features).\n",
    "Here, both hᵀ and cᵀ would have the last states of all the k layers in the stack.\n",
    "Further at initiation, had we set batch_first=True,\n",
    "then the timesteps and batch dimensions would swap in the output (similar to the input).\n",
    "'''\n",
    "#(seq_length * batch_size * hidden_size)\n",
    "\n",
    "p_lstm_test[0].shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "'''\n",
    "TF LSTM layer expects a 3 dimensional tensor as input during forward propagation.\n",
    "This input should be of the shape (batch, timesteps, input_features)\n",
    "'''\n",
    "k_lstm = tf.keras.layers.LSTM(\n",
    "        rnn_units,\n",
    "        return_sequences=True,\n",
    "        recurrent_initializer='glorot_uniform',\n",
    "        recurrent_activation='sigmoid',\n",
    "        stateful=True,\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "'''\n",
    "Get output from keras LSTM\n",
    "'''\n",
    "k_lstm_test = k_lstm(k_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "'''\n",
    "Make a keras dense layer\n",
    "'''\n",
    "k_dense = tf.keras.layers.Dense(units=vocab_size)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "'''\n",
    "Get output of keras dense layer\n",
    "'''\n",
    "k_out = k_dense(k_lstm_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "'''\n",
    "Create a PyTorch linear (dense) layer\n",
    "'''\n",
    "p_dense = torch.nn.Linear(rnn_units, vocab_size)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "'''\n",
    "Output for PyTorch LSTM is a tuple (tensor, (tensor, tensor)) so take the first element of the tuple and pass into dense layer\n",
    "'''\n",
    "p_out = p_dense(p_lstm_test[0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[13.765961   4.1679363  3.5024328 14.174576   3.3851292  3.3538055\n",
      "   3.146059  14.424006  14.415008  14.439827  14.352431  14.454153\n",
      "  14.51372    3.8280933 14.532884  14.395902  14.468557   4.229684\n",
      "  14.568549   6.37278    5.405327   3.3354204 14.299489  14.591692\n",
      "   4.0014505  4.0593195  3.7824407  2.9956534  4.916977  14.780802\n",
      "   3.3925154  2.8981307  5.1328225  3.1150978  5.724106   4.9540133\n",
      "  14.750332   3.706162   3.3901477  3.316679   3.1679006 14.707722\n",
      "  14.5920315 14.462907   3.9426234 14.373063  14.308451  14.228557\n",
      "  14.253304  14.494127   2.3596013 14.146403  14.26072    4.4960294\n",
      "  14.46563   14.403555  14.448816   2.8844917 14.497851   3.6454391\n",
      "   4.5588217  2.6133156 14.214947  14.333407   2.7033124 14.382575\n",
      "  14.392125  14.413561  14.449843  14.576517  14.523473   3.932469\n",
      "   3.298026  14.470236  14.435741  14.564459  14.494001   5.1289854\n",
      "  14.500172  14.348515  14.478828  14.441821  14.537241  14.597513\n",
      "  14.715878  14.528492  14.674841  14.527171  14.66786    3.8926055\n",
      "   4.5193706  2.9803805 14.707113   6.207043   3.2862487 14.815538\n",
      "   3.7857828 14.532263   4.7683706  4.9358654]\n",
      " [14.093409   3.1319997 14.088419   3.9401445  4.0745344  4.3236847\n",
      "   3.676291   3.3860943  3.875661  14.565855  14.481998  14.434441\n",
      "  14.485253   3.5519686  3.71222    4.00037   14.475714   2.6079009\n",
      "  14.479091   4.9307346  3.9910119  4.105256   3.7320247  3.8245246\n",
      "  14.411151  14.4173     4.737262  14.454744   4.892475  14.572807\n",
      "   4.172269   3.778315  14.506424   4.414646   3.0904593 14.566728\n",
      "   5.2730675 14.567537   3.606287  14.565744  14.407467  14.499988\n",
      "  14.500262  14.532948  14.563412  14.425969   3.8833666  5.2971573\n",
      "   6.670904  14.647558  14.517668   4.7795477  5.51763    3.67924\n",
      "   4.7894335 14.570461  14.598612   4.257928  14.668098   5.0142474\n",
      "   4.8480544  3.113108   4.266592  14.279572   3.9846768  3.0706022\n",
      "  14.422326   4.296958  14.295802   4.4274955  2.8004448 14.100779\n",
      "  14.388213   3.1266112 14.436336   5.6536074 14.452736  14.436898\n",
      "   3.574911  14.440102  14.221609  14.018026  14.059005   2.772288\n",
      "  14.295496  14.019462   3.0851922 14.106613  14.054607   4.8665614\n",
      "   6.498864  14.382397   3.812183  14.284287   3.4005404 14.316021\n",
      "   3.7200103 14.2233715 14.3718    14.251815 ]\n",
      " [ 3.2307262 14.04591    3.1242924 14.182135   4.622953  14.369959\n",
      "  14.33056    3.0897517 14.189747   3.644845   5.12575   14.236591\n",
      "   3.7223248 14.508245  14.553734   3.3048503  3.6689525  3.4997182\n",
      "   3.177682   2.7390027 14.268219  14.440458  14.339496  14.18455\n",
      "  14.197954   3.1731288 14.337954  14.286039   4.2892585  4.5486956\n",
      "   6.1500015 14.497505   5.1534557  3.7276816  4.0133944 14.308783\n",
      "  14.375555  14.375526  14.519643   4.678295   3.632688  14.866553\n",
      "   4.4860973 14.736866  14.674395   5.159909  14.641049  14.566505\n",
      "   3.777783  14.708457   3.6861165  7.885891   3.892379   4.0130033\n",
      "   5.145092   3.9313579  4.103242   3.7318919 14.275965  14.410882\n",
      "   5.293348  14.362968   4.028469   6.369485  14.521599   3.8369517\n",
      "  14.264779   3.7737072 14.3357     3.4082632  3.9948492  5.217991\n",
      "  14.647348   4.19605    3.01369    2.8437488  3.7649617 14.419786\n",
      "  14.553005   6.487714  14.6353445 14.581821   4.937723   4.3558874\n",
      "  14.662132  14.417817  14.440593  14.462296   5.1550145  3.6097474\n",
      "  14.522289   3.725056   7.774275  14.431257  14.515331  14.739115\n",
      "   3.5762131 14.742496  14.779177  14.71465  ]\n",
      " [13.9399605 14.167765  14.346588  14.378777  14.339869  14.3353\n",
      "  14.305758   3.0634677 14.452447   4.666326  14.306009  14.421547\n",
      "  14.466702   6.048234  14.428249   2.878032   5.980901  14.691875\n",
      "   3.5154939 14.79457    4.0234537  4.742011   8.251415  14.443848\n",
      "  14.486996   3.223679  14.735308  14.671367   5.01827   14.40314\n",
      "  14.459147  14.416855  14.461     14.409345   3.5606961  2.9654508\n",
      "  14.354308  14.352385   5.6423903 14.487035   3.0380034  4.670703\n",
      "   2.9688115 14.453327   4.9950185 14.485509  14.496025  14.441193\n",
      "  14.507458   3.3098953 14.461651   3.9407299  7.2556605 14.432026\n",
      "  14.412247   3.7916     4.4106174 14.45159    4.6686783  4.94753\n",
      "   3.4954424  4.2594266 14.375983  14.341761  14.267449  14.393631\n",
      "  14.25226    4.205468   3.873378   4.42348    3.0510433  3.315333\n",
      "  14.577176   3.5188835  3.8803494 14.712721  14.784498   4.0540133\n",
      "   3.3779309  3.2454371  3.358776   3.7689025 14.500196  14.55079\n",
      "   4.0800476  3.9134686 14.586981  14.704613   3.2506602  3.2521746\n",
      "  14.603966   4.4329743 14.558494  14.543742   3.3534157  4.3781238\n",
      "   4.7653437 14.483797   3.225136   5.4151983]], shape=(4, 100), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Create some random labels to calculate loss function. We're using sparse_categorical_crossentropy in keras\n",
    "and CrossEntropyLoss in PyTorch\n",
    "'''\n",
    "\n",
    "labels = [[randint(0, vocab_size-1) for j in range(seq_length)] for i in range(batch_size)]\n",
    "#print(labels)\n",
    "loss = tf.keras.losses.sparse_categorical_crossentropy(labels, k_out, from_logits=False)\n",
    "print(loss)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.0354,  0.0064, -0.0296,  ..., -0.0619,  0.0011,  0.1172],\n",
      "         [-0.0968, -0.0472,  0.0209,  ..., -0.0288, -0.0691, -0.0899],\n",
      "         [-0.0085, -0.0450,  0.0250,  ...,  0.0522, -0.0569, -0.0256],\n",
      "         ...,\n",
      "         [ 0.0208, -0.0147,  0.0275,  ..., -0.0376, -0.0263, -0.0321],\n",
      "         [ 0.0405,  0.0866,  0.0260,  ..., -0.0834, -0.0638, -0.0335],\n",
      "         [-0.0341,  0.0209, -0.0576,  ..., -0.0285,  0.0321, -0.0439]],\n",
      "\n",
      "        [[ 0.0194,  0.0387,  0.0793,  ..., -0.0215,  0.0194,  0.0167],\n",
      "         [-0.0249,  0.0300, -0.0852,  ..., -0.0272, -0.0147,  0.0240],\n",
      "         [-0.0398,  0.0186,  0.0375,  ..., -0.0213, -0.0470, -0.0081],\n",
      "         ...,\n",
      "         [ 0.0354, -0.0178, -0.0602,  ...,  0.0076, -0.0272, -0.0084],\n",
      "         [-0.0127, -0.0078, -0.0472,  ..., -0.0145,  0.0718, -0.0316],\n",
      "         [ 0.0278, -0.0214, -0.0678,  ..., -0.0419, -0.0032, -0.0162]],\n",
      "\n",
      "        [[ 0.0057, -0.0062,  0.0109,  ...,  0.0142,  0.0307, -0.0475],\n",
      "         [-0.0299, -0.0326, -0.0675,  ..., -0.0092,  0.0285, -0.0761],\n",
      "         [-0.0525, -0.0280, -0.0808,  ...,  0.0628,  0.0821,  0.0334],\n",
      "         ...,\n",
      "         [ 0.0525,  0.0915,  0.0387,  ..., -0.0497,  0.0106,  0.0374],\n",
      "         [-0.0279, -0.0688, -0.0450,  ...,  0.0140, -0.0130,  0.0183],\n",
      "         [-0.0301, -0.0218,  0.0123,  ..., -0.0262, -0.0689, -0.0062]],\n",
      "\n",
      "        [[ 0.0305,  0.0163,  0.0230,  ..., -0.0219,  0.0041,  0.0058],\n",
      "         [ 0.0213,  0.0335, -0.0140,  ..., -0.0525, -0.0358, -0.0153],\n",
      "         [-0.1158, -0.0665, -0.0647,  ...,  0.0752, -0.0262,  0.0279],\n",
      "         ...,\n",
      "         [ 0.0681,  0.0197,  0.0043,  ..., -0.0119,  0.0207, -0.0227],\n",
      "         [ 0.0307,  0.0410, -0.0237,  ..., -0.0134, -0.0630, -0.0320],\n",
      "         [ 0.1087,  0.0711,  0.0115,  ...,  0.0836,  0.0435, -0.0235]]],\n",
      "       grad_fn=<PermuteBackward0>)\n",
      "tensor([[46, 10, 80, 19, 44, 72, 40, 35, 50, 44, 24, 68, 12, 25, 77, 73, 53, 65,\n",
      "         21, 82, 71, 33, 53, 17, 74, 74, 60, 77, 81, 46, 30, 45,  7, 74, 66, 26,\n",
      "         65, 35, 32, 60, 15, 76, 12, 63, 32, 47, 40,  2, 14, 41, 61, 40, 19, 43,\n",
      "         36, 32, 57, 44, 56, 18, 36, 15, 77, 55, 30, 77, 80, 72,  6, 53, 32, 25,\n",
      "         43, 61, 55, 31, 32,  4, 67, 46, 57, 65, 15, 59, 50, 11, 56, 62, 21, 79,\n",
      "         24, 82, 14, 74, 45, 41, 22,  4, 61, 54],\n",
      "        [ 8, 54, 24, 50, 38, 29, 70, 31,  8, 46, 28, 22, 22, 23, 30, 26, 42,  3,\n",
      "         41,  2, 41, 47, 31, 64, 33, 17, 35, 71, 56, 69, 41, 54,  7, 64, 66, 31,\n",
      "         22, 78, 58, 61, 67, 24, 15, 34, 65, 38, 59, 24, 23, 11, 78, 34,  8, 35,\n",
      "         17,  8, 10, 24, 33, 52, 48, 74, 82,  0, 45, 74,  7, 22, 17, 45, 27, 26,\n",
      "          4, 29, 21, 72, 67, 61,  2, 37, 15, 13, 19, 60, 11, 64, 82, 46, 22, 37,\n",
      "         44, 76, 40,  3, 25, 11,  8, 57, 78, 46],\n",
      "        [23, 49, 29,  6, 73, 14, 61, 28, 31, 26, 35, 82, 78,  0, 14, 49, 59, 44,\n",
      "         30, 82, 45, 34, 40, 61,  8, 71, 11, 75, 21, 29, 19, 11, 43, 45,  7, 65,\n",
      "          8, 42, 15, 65, 31, 30, 72, 10, 12, 56, 61, 30, 66, 53, 66, 81, 67, 63,\n",
      "         18,  2, 40, 29, 33, 80,  2, 13, 66, 48, 73,  3, 21, 40, 39, 32, 48,  5,\n",
      "         14, 70, 66, 16, 57, 46, 52, 15, 40, 55, 59, 63, 14, 30, 11, 81, 14, 56,\n",
      "         16, 60, 41, 38,  6, 64, 55, 12, 67, 37],\n",
      "        [70, 75, 11, 79, 13, 18, 29, 37, 57, 20, 76, 50, 11, 73, 41, 60,  2, 27,\n",
      "         51,  4, 71, 14, 67, 33, 27, 30, 35, 73, 25, 23, 36, 53, 77, 78, 61,  3,\n",
      "         51, 42, 77, 11, 36,  9, 22, 81, 58, 65, 37, 55, 73, 40, 51, 20, 57, 55,\n",
      "         15, 62, 82, 64,  6, 72, 31, 42, 59, 69, 48, 74,  6, 11, 48, 52,  3, 64,\n",
      "         71, 25, 68, 27, 75, 56, 76, 10, 24, 42, 29,  1, 34, 71, 33,  1, 63, 30,\n",
      "         61, 80, 78, 62, 10, 17,  2, 38, 40, 25]])\n"
     ]
    },
    {
     "data": {
      "text/plain": "tensor([[4.4556, 4.4446, 4.3917, 4.4269, 4.4474, 4.4787, 4.3162, 4.4109, 4.4482,\n         4.4008, 4.3853, 4.3863, 4.3461, 4.3837, 4.3364, 4.3408, 4.4233, 4.3968,\n         4.3867, 4.3710, 4.4553, 4.3982, 4.3778, 4.4252, 4.4640, 4.4664, 4.4299,\n         4.4188, 4.3705, 4.4602, 4.4568, 4.3845, 4.4133, 4.4565, 4.4071, 4.3972,\n         4.4566, 4.3954, 4.4144, 4.3814, 4.2896, 4.4201, 4.3011, 4.5056, 4.4360,\n         4.4831, 4.4163, 4.4677, 4.5258, 4.4243, 4.4262, 4.5003, 4.5333, 4.4032,\n         4.3575, 4.4255, 4.4375, 4.3780, 4.4160, 4.4436, 4.4497, 4.4436, 4.5374,\n         4.4290, 4.4244, 4.4642, 4.4132, 4.4152, 4.3564, 4.3603, 4.4069, 4.3782,\n         4.4222, 4.4271, 4.4165, 4.4453, 4.4194, 4.3374, 4.4369, 4.4376, 4.4419,\n         4.4414, 4.4036, 4.3221, 4.4511, 4.4091, 4.4005, 4.4421, 4.4221, 4.4580,\n         4.5162, 4.4325, 4.4830, 4.4016, 4.2722, 4.4357, 4.4149, 4.3777, 4.3639,\n         4.4577],\n        [4.4158, 4.4058, 4.4664, 4.4990, 4.4742, 4.4212, 4.4077, 4.4694, 4.3952,\n         4.3903, 4.3785, 4.4775, 4.3996, 4.3869, 4.4667, 4.3989, 4.3640, 4.3896,\n         4.4291, 4.3937, 4.3965, 4.4251, 4.4417, 4.3678, 4.3986, 4.4957, 4.4339,\n         4.4495, 4.3761, 4.3178, 4.3653, 4.4237, 4.3451, 4.5086, 4.4588, 4.3840,\n         4.4223, 4.4829, 4.3896, 4.3632, 4.4242, 4.4279, 4.4113, 4.4152, 4.4212,\n         4.3989, 4.4641, 4.3104, 4.4210, 4.4211, 4.4570, 4.4335, 4.3420, 4.3307,\n         4.4349, 4.3600, 4.4264, 4.4400, 4.4156, 4.5333, 4.3975, 4.4281, 4.4603,\n         4.4144, 4.4289, 4.4493, 4.3921, 4.3494, 4.5024, 4.3202, 4.4554, 4.4038,\n         4.3219, 4.4358, 4.3942, 4.4701, 4.4646, 4.4310, 4.4289, 4.3888, 4.3312,\n         4.4638, 4.4537, 4.4528, 4.4933, 4.4158, 4.4669, 4.3933, 4.4593, 4.3685,\n         4.4039, 4.4300, 4.4268, 4.3864, 4.3931, 4.4062, 4.3704, 4.4938, 4.5393,\n         4.3534],\n        [4.4233, 4.3682, 4.3946, 4.3737, 4.3418, 4.4700, 4.4358, 4.4251, 4.4828,\n         4.4308, 4.4427, 4.4019, 4.3769, 4.4698, 4.5091, 4.4380, 4.3488, 4.5087,\n         4.4626, 4.3919, 4.3662, 4.4163, 4.5435, 4.4512, 4.4847, 4.3844, 4.3940,\n         4.4385, 4.3670, 4.4421, 4.4254, 4.4585, 4.3752, 4.3883, 4.4479, 4.4310,\n         4.4512, 4.3711, 4.4144, 4.3792, 4.5159, 4.4654, 4.5070, 4.4781, 4.3961,\n         4.4224, 4.3438, 4.3952, 4.4439, 4.4905, 4.4130, 4.4691, 4.4564, 4.4941,\n         4.3781, 4.3621, 4.4748, 4.3610, 4.3584, 4.3863, 4.3659, 4.4654, 4.4643,\n         4.4202, 4.3546, 4.3042, 4.3974, 4.4232, 4.4673, 4.3857, 4.4720, 4.4173,\n         4.4286, 4.4259, 4.4763, 4.4229, 4.4246, 4.3614, 4.4198, 4.4394, 4.3901,\n         4.3901, 4.4619, 4.5224, 4.5037, 4.4067, 4.3900, 4.3819, 4.5086, 4.3374,\n         4.4748, 4.4448, 4.3468, 4.4366, 4.4758, 4.3710, 4.4300, 4.3355, 4.4149,\n         4.4172],\n        [4.4440, 4.4086, 4.3096, 4.3900, 4.4680, 4.3526, 4.4184, 4.4703, 4.4569,\n         4.4150, 4.4725, 4.4078, 4.4385, 4.4175, 4.4342, 4.4368, 4.3497, 4.4728,\n         4.3484, 4.3255, 4.3202, 4.4109, 4.3727, 4.4189, 4.3865, 4.4503, 4.4898,\n         4.3902, 4.3486, 4.4222, 4.3548, 4.4011, 4.3807, 4.4740, 4.4036, 4.4137,\n         4.4879, 4.3980, 4.4537, 4.3783, 4.3849, 4.4623, 4.3765, 4.3620, 4.4009,\n         4.3707, 4.4424, 4.3690, 4.3244, 4.4683, 4.4373, 4.4778, 4.4569, 4.3699,\n         4.4339, 4.3234, 4.4481, 4.4718, 4.4463, 4.5191, 4.4548, 4.4057, 4.4219,\n         4.4326, 4.4339, 4.4399, 4.4436, 4.3858, 4.4412, 4.4220, 4.3726, 4.3628,\n         4.4260, 4.4183, 4.4472, 4.4112, 4.4179, 4.4045, 4.4353, 4.4509, 4.4073,\n         4.4295, 4.3484, 4.4761, 4.4208, 4.3119, 4.4328, 4.4439, 4.3600, 4.3581,\n         4.3876, 4.4615, 4.3970, 4.3632, 4.4640, 4.4842, 4.4219, 4.3852, 4.3725,\n         4.3323]], grad_fn=<ViewBackward0>)"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "We have to change the shape of the output from the dense layer\n",
    "'''\n",
    "\n",
    "x=torch.Tensor(p_out).permute((0,2,1))  # shape of preds must be (N, C, H, W) instead of (N, H, W, C)\n",
    "y=torch.Tensor(labels).long() #  shape of labels must be (N, H, W) and type must be long integer\n",
    "print(x)\n",
    "print(y)\n",
    "losses = nn.CrossEntropyLoss(reduction=\"none\")(x, y)\n",
    "losses"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}